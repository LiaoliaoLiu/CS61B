# Lecture
- [$2^{floor(\log_2 N) + 1} - 1$](https://youtu.be/sFUkCiswzXc?list=PL8FaHk7qbOD5Ek10eT39UqAjcwr99xqZP&t=274)
- [No magic shortcul for runtime problems](https://youtu.be/sFUkCiswzXc?list=PL8FaHk7qbOD5Ek10eT39UqAjcwr99xqZP&t=274)
  - Strategies: Find exact sum. Write out examples. Draw pictures.
- [Recursion Exact Counting](https://youtu.be/Ht6ySSoC0FM?list=PL8FaHk7qbOD5Ek10eT39UqAjcwr99xqZP&t=101)
- [There was a subtle bug in Java's official binary search (2006)](https://youtu.be/RfoP3xULk70?list=PL8FaHk7qbOD5Ek10eT39UqAjcwr99xqZP&t=222)
- [Binary Search Exact Count](https://youtu.be/SPX408bkhgU?list=PL8FaHk7qbOD5Ek10eT39UqAjcwr99xqZP&t=217)
- [Handy Big Theta Properties](https://youtu.be/SPX408bkhgU?list=PL8FaHk7qbOD5Ek10eT39UqAjcwr99xqZP&t=322)
  - The floor of f has the same order of growth as f
  - The ceiling of f has the same order of growth as f
  - Logarithm base does not affect order of growth (change of base $\log_2 N = \frac{log_{10}N}{log_{10}2}$, $\log_{10}2$ is a constant)
- [Arbitrary units of time (AU) is not rigorous but handy and intuitive](https://youtu.be/3aRCQJxGwCQ?list=PL8FaHk7qbOD5Ek10eT39UqAjcwr99xqZP&t=123)
- [The Merge Operation](https://youtu.be/3aRCQJxGwCQ?list=PL8FaHk7qbOD5Ek10eT39UqAjcwr99xqZP&t=179)
  - Given two sorted arrays, the merge operation combines them into a single sorted arrary by successively copying the smallest item from the two arrays into a target array.
- [Why Merge Sort is $N\log N$ (Linearithmic)](https://youtu.be/AlwAZkqzHqI?list=PL8FaHk7qbOD5Ek10eT39UqAjcwr99xqZP&t=296)
  - $N^2$ vs. $N\log N$ is an enormous difference.
  - Going from $N\log N$ to $N$ is nice, but not a radical change.
- [Understanding every pattern takes time, but what you need to know is in this lecture](https://youtu.be/keUNAiiGVy8?list=PL8FaHk7qbOD5Ek10eT39UqAjcwr99xqZP&t=43)

# Overview
**Runtime Analysis.** Understanding the runtime of code involves deep thought. It amounts to asking: “How long does it take to do stuff?”, where stuff can be any conceivable computational process whatsoever. It simply cannot be done mechanically, at least for non-trivial problems. As an example, a pair of nested for loops does NOT mean $\Theta(N^2)$ runtime as [we saw in lecture](https://www.youtube.com/watch?v=sFUkCiswzXc&list=PL8FaHk7qbOD5Ek10eT39UqAjcwr99xqZP&t=274s).

**Cost Model.** As an anchor for your thinking, recall the idea of a “cost model” from last lecture. Pick an operation and count them. You want the one whose count has the highest order of growth as a function of the input size.

**Important Sums.** This is not a math class so we’ll be a bit sloppy, but the two key sums that you should know are that:
- $1+2+3+…+N \in \Theta(N2)$
- $1+2+4+8+…+N \in \Theta(N)$ ($2^{floor(\log_2 N) + 1} - 1$ Don't take N as the nth item in the serie)

**Practice.** The only way to learn this is through plenty of practice. Make sure to work through the problems in lecture and below when you have some time.

# Recommended Problems
## C level
### 1
Q: Prove that $O(N+\frac{N}{2}+\frac{N}{4}+….2+1)=O(N)$ (hand wavy proof is okay as long as you gain the intuition)

A: Use the sum of geometric series $a (\frac{1-r^n}{1-r})$ where $n = \log_2 N + 1$ and $r = 2$
- n stands for the postion index (nth item), 
- the nth item is $N$, 
- every item obeys $a(n) = 2^{n-1}$,
- $a(n) = N = 2^{n-1}$
- $n = \log_2 N + 1$

### 2
Q: What would the runtime of `modified_fib` be. Assume that values is an array of size n. If a value in an int array is not initialized to a number, it is automatically set to 0.
```java
 public void modified_fib(int n, int[] values){
   if(n <= 1){
     values[n] = n;
     return n;
   }
   else{
     int val = values[n];
     if(val == 0){
       val = modified_fib(n-1, values) + modified_fib(n-2, values);
       values[n] = val;
     }
     return val;
   }
 }
```
A: $\Theta(N)$. `val` would be calculated by N-2 times;

### 3
Q: Prove to yourself that $\Theta(\log_2(n))=\Theta(log_3(n))$

## B level
### 1
Q: Find the runtime of running `print_fib` with for arbitrary large n.
```java
 public void print_fib(int n){
   for(int i = 0; i < n; i++){  // I believe here is a typo.
       System.out.println(fib(i));
   }
 }

 public int fib(int n){
   if(n <= 0){
     return 0;
   }
   elif(n == 1){
     return 1;
   }
   else{
     return fib(n-1) + fib(n-2);
   }
 }
```
A: $\Theta(n 2^n)$ by [Arithmetico–geometric sequence](https://en.wikipedia.org/wiki/Arithmetico%E2%80%93geometric_sequence)

### 2
Q: Do problem 5 again, but change the body of the for loop in print_fib to be
```java
 System.out.println(fib(n));
```
A: The same.

### 3
Q: Find the runtime of this function
```java
 public void melo(int N){
   for(int i = 0; i < N*N; i++){
     System.out.println("Gelo is fruit pudding");
   }
   for(int i = 0; i < N*N*N; i++){
     System.out.println("Zo Two the Warriors");
   }
 }
```
A: $\Theta(N^3)$

### 4
Q: Find the runtime of this function
```java
 public void grigobreath(int N){
     if(N==0){
       return;
     }
     for(int i  = 0; i < N; i++){
       System.out.println("Gul-great")
     }
     grigobreath(N * 1/2);
     grigobreath(N * 1/4);
     grigobreath(N * 1/4);
 }
```
A: $\Theta(N\log N)$. Note that the times of loop get halved ever recursive call.

### 5
> I don't need to take exam and I just give up lol.

Q: [Problem 8](https://tbp.berkeley.edu/exams/6137/download/) from Spring 2018 midterm #2

### 6
Q: [Problem 4](https://tbp.berkeley.edu/exams/5773/download/) from Spring 2017 midterm #2

# Q&A
- [Is DisjointedSet used every frequently](https://youtu.be/Wsb9kP59VS4?t=371)
  - It's a somewhat obscure data structure.
  - Limited usage, because it only determines connectivity not how things are connected.
  - It's a data structure to support a number of ther algorithms (minimum spanning tree of a graph).
  - I put it in the lecture first because it's weird and you don't see it normally.
- [Cardinal rule: don't try and step the recursive code in the visualizer](https://youtu.be/Wsb9kP59VS4?t=805)
- [Space useage about merge sort](https://youtu.be/Wsb9kP59VS4?t=1569)
  - The most naive possible way it's not that much memory.
  - The total memory useage would be **the entire left half**.
  - There is a [trick](https://algs4.cs.princeton.edu/22mergesort/MergeX.java.html) that each recursive call you swap the order of the front and two array arguments. It saves you time not memory.
- [Quick Union with path compression you don't need weighted for it to work well](https://youtu.be/Wsb9kP59VS4?t=1863)

# Asymptotics II
## Recursion
Consider the function f3:
```java
public static int f3(int n) {
   if (n <= 1) 
      return 1;
   return f3(n-1) + f3(n-1);
}
```

![recursion](https://joshhug.gitbooks.io/hug61b/content/assets/asymptotics2_tree.png)

Our general form then is: 
$C(N) = 1 + 2 + 4 + ... + 2^{N-1}$

And this should start to look a bit familiar. Above we saw **the sum of the first powers of 2**:
$1 + 2 + 4 + 8 + ... + Q = 2Q - 1$

In this case, $Q = 2^{N-1}$.

So, $C(N) = 2Q - 1 = 2(2^{N-1}) - 1 = 2^N - 1$

## Binary Search

For an animation of binary search, see [these slides](https://docs.google.com/presentation/d/1P4HKmsO3Aaugv7_U16jJN0UbfTEJi1uZUdi_WbIIGe0/edit#slide=id.g463de7561_042).

**Example Proof:** Prove $\lfloor f(N) \rfloor = \Theta (f(N))$

**Solution:** $f(N) - 1/2 < f(N) \leq \lfloor f(N) + 1/2 \rfloor \leq f(N) + 1/2$ Simplifying $f(N) + 1/2$ and $f(N) - 1/2$ according to our big theta rules by dropping the constants, we see that they are of order $f(N)$. Therefore $\lfloor f(N) + 1/2 \rfloor$ is bounded by two expressions of order $f(N)$, and is therefore also $\Theta ( f(N) )$

**Exercise:** Prove $\lceil f(N) \rceil = \Theta (f(N))$
**Exercise:** Prove $\log_p(N) = \Theta (\log_q(N))$

**One cool fact to wrap up with:** Log time is super good! It's almost as fast as constant time, and way better than linear time. This is why we like binary search, rather than stepping one by one through our list and looking for the right thing.

## Merge Sort

The development logic:
- Selection Sort is $\Theta(N^2)$
- [Merging two arrays](https://docs.google.com/presentation/d/1P4HKmsO3Aaugv7_U16jJN0UbfTEJi1uZUdi_WbIIGe0/edit#slide=id.g1d0770ba4a_0_14) is $\Theta(N)$
- The runtime for split-in-half-then-merge-them sort is $O(2(\frac{N}{2})^2 + N)$, which cloud bring better performance.
- Do it again and again.
- Eventually we'll reach lists of size 1. At that point, we don't even have to use selection sort, because a list with one element is already sorted.

This is the essence of merge sort:
- If the list is size 1, return. Otherwise:
- Mergesort the left half
- Mergesort the right half
- Merge the results

Mergesort has worst case runtime = $\Theta(N \log N)$.
- The top level takes ~N AU.
- Next level takes ~N/2 + ~N/2 = ~N.
- One more level down: ~N/4 + ~N/4 + ~N/4 + ~N/4 = ~N.

Thus, total runtime is ~Nk, where k is the number of levels.

How many levels are there? We split the array until it is length 1, so $k = log_2(N)$. Thus the overall runtime is $\Theta (N log N)$.

# Omega and Amortized Analysis
This section expands on the concept of Big O and introduces Omega. We'll also explore the idea of amortized runtimes and their analysis. Finally, we'll end on empirical analysis of runtimes and a sneak preview of complexity theory.

## Runtime Analysis Subtleties
Big Theta expresses the exact order of as a function of the input size. However, if the runtime **depends on more than just the size** of the input, then we must qualify our statements into different cases before using Big Theta. Big O does away with this annoyance. Rather than having to describe both the best and worse case, for the example above, we can simply say that the runtime of dup4 is $O(N^2)$. Sometimes dup4 is faster, but it's at worst quadratic.

## Big O Abuse
Consider the following statements:
1. The most expensive room in the hotel is $639 per night.
2. Every room in the hotel is less than or equal to $639 per night.

The first one provides an exact upper bound (not only the upper bound of room prices, but also that this upper bound is reached.). However, in the second statement, the most expensive room **could be less than $639**.

**Exercise:** Which statement gives you more information about the runtime of a piece of code?
1. The worst case runtime is Θ(N^2).
2. The runtime is O(N^2). 

**Answer:** Similar to the hotel problem, the first statement provides more information. Consider the following method:
```java
public static void printLength(int[] a) {
    System.out.println(a.length);
}
```
Both this simple method and dup4 have runtime $O(N^2)$, so knowing statement 2 would not be able to distinguish between these. But statement 1 is more precise, and is only true for dup4.

**Note:** Big O is NOT the same as "worst case". But it is often used as such.

To summarize the usefulness of Big O:

- It allows us to make simple statements without case qualifications, in cases where the runtime is different for different inputs.
- Sometimes, for particularly tricky problems, we (the computer science community) don't know the exact runtime, so we may only state an upper bound. (Find the shortest route is $O(2^n)$)
- It's a lot easier to write proofs for Big O than Big Theta, like we saw in finding the runtime of mergesort in the previous chapter. This is beyond the scope of this course.

## Big Omega
Big Omega describes lower bounds.

There's two common uses for Big Omega:

- It's used to prove Big Theta runtime. If $R(N) = O(f(N))$ and $R(N) = \Omega(f(N))$, then $R(N) = \Theta(f(N))$. Sometimes, it's easier to prove O and $\Omega$ separately. This is outside the scope of this course.
- It's used to prove the difficulty of a problem. For example, ANY duplicate-finding algorithm must be $\Omega(N)$, because the algorithm must at least look at each element.

## Amortized Analysis (Rigorous Explanation)
A more rigorous examination of amortized analysis is done here, in three steps:

1. Pick a cost model (like in regular runtime analysis)
2. Compute the average cost of the i'th operation
3. Show that this average (amortized) cost is bounded by a constant.

We'll now introduce the idea of "potential" to aid us in solving this amortization mystery. For each operation i, eg. each add or Grigometh visit, let $c_i$ be the true cost of the operation, while $a_i$ be some arbitrary amortized cost of the operation. $a_i$, a constant, must be the same for all i.

Let $\Phi_i$ be the potential at operation i, which is the cumulative difference between amortized and true cost: $\Phi_i = \Phi_{i-1} + a_i - c_i$

$a_i$ is an arbitrary constant, meaning we can chose it. **If we chose $a_i$ such that $\Phi_i$ is never negative and $a_i$ is constant for all ii, then the amortized cost is an upper bound on the true cost.** And if the true cost is upper bounded by a constant, then we've shown that it is on average constant time!

**Exercise:** What is the value of $c_i$ for ArrayList add operations? If we let the amortized cost $a_i = 5$, will the potential ever become negative? Is there a smaller amortized cost that works? Fill out a table like the one for Grigometh to help with this.

**Answer:** $c_i$ is the total cost for array resizing and adding the new element, where $c_i = 2^i + 1$ if i is a power of 2, and $c_i = 1$ otherwise.

## Summary

- Big O is an upper bound ("less than or equals")
- Big Omega is a lower bound ("greater than or equals")
- Big Theta is both an upper and lower bound ("equals")
- Big O does NOT mean "worst case". We can still describe worst cases using Big Theta
- Big Omega does NOT mean "best case". We can still describe best cases using Big Theta
- Big O is sometimes colloquially used in cases where Big Theta would provide a more precise statement
- Amortized analysis provides a way to prove the average cost of operations.
- If we chose $a_i$ such that $\Phi_i$ is never negative and $a_i$​ is constant for all $i$, then the amortized cost is an upper bound on the true cost.